{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thomas-Land/car/blob/main/%E6%B1%BD%E8%BD%A6%E7%94%A8%E6%88%B7%E6%BB%A1%E6%84%8F%E5%BA%A6%E5%B1%82%E6%AC%A1%E5%8C%96%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%A8%A1%E5%9E%8B_(%E4%BB%BB%E5%8A%A11_%E5%8A%A8%E6%80%81%E5%9B%A0%E5%AD%90%E5%88%86%E7%BB%84).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=FutureWarning) # Ignore some pandas warnings\n",
        "\n",
        "# --- 1. 数据加载与结构解析 ---\n",
        "data_file = 'D:/新建文件夹/2/案例game/案例第一题数据.xlsx'\n",
        "# 加载数据\n",
        "data_df = pd.read_excel(data_file,sheet_name='数据')\n",
        "factor_desc_df = pd.read_excel(data_file,sheet_name='因子说明')\n",
        "structure_df = pd.read_excel(data_file, sheet_name='结构说明')\n",
        "print(\"数据概览 (前5行):\")\n",
        "print(data_df.head())\n",
        "print(f\"\\n数据形状: {data_df.shape}\")\n",
        "print(\"\\n因子说明:\")\n",
        "print(factor_desc_df.head())\n",
        "print(\"\\n结构说明:\")\n",
        "print(structure_df)\n",
        "\n",
        "# --- 动态构建因子分组 ---\n",
        "print(\"\\n动态构建因子分组:\")\n",
        "high_level_factors_map = {}\n",
        "try:\n",
        "    # 假设 '因子说明.csv' 有 '一级指标' 和 '变量名' 列\n",
        "    # 假设 '变量名' 列包含的是 'case1_data.csv' 中的实际列名\n",
        "    # !! 如果您的列名不同，请修改这里的 '一级指标' 和 '变量名' !!\n",
        "    required_cols_factor = ['一级指标', '变量名']\n",
        "    if not all(col in factor_desc_df.columns for col in required_cols_factor):\n",
        "        print(f\"错误: '因子说明.csv' 文件缺少必要的列: {required_cols_factor}。请检查文件。\")\n",
        "        exit()\n",
        "\n",
        "    # 按 '一级指标' 分组，并将对应的 '变量名' 收集成列表\n",
        "    grouped = factor_desc_df.groupby('一级指标')['变量名'].apply(list)\n",
        "    high_level_factors_map = grouped.to_dict()\n",
        "\n",
        "    print(\"根据 '因子说明.csv' 构建的初步分组:\")\n",
        "    for name, cols in high_level_factors_map.items():\n",
        "        print(f\"- {name}: 找到了 {len(cols)} 个变量名 (例如: {cols[:3]}...)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"错误: 从 '因子说明.csv' 构建分组时出错: {e}\")\n",
        "    print(\"将尝试使用之前的假设性分组，但这很可能不正确。\")\n",
        "    # Fallback to previous assumption (likely incorrect based on user feedback)\n",
        "    high_level_factors_map = {\n",
        "        '质量可靠性': [f'X{i}' for i in range(1, 12)],\n",
        "        '性能设计': [f'X{i}' for i in range(12, 139)],\n",
        "        '销售服务': [f'S{i}' for i in range(1, 85)],\n",
        "        '售后服务': [f'A{i}' for i in range(1, 59)]\n",
        "    }\n",
        "\n",
        "\n",
        "# 过滤掉数据中不存在的列名 (基于动态或回退后的映射)\n",
        "valid_factor_groups = {}\n",
        "all_low_level_columns = []\n",
        "print(\"\\n检查并过滤因子分组中的列:\")\n",
        "for group_name, columns in high_level_factors_map.items():\n",
        "    # 确保列名是字符串类型\n",
        "    columns = [str(col) for col in columns]\n",
        "    # 检查数据列名是否存在 (也确保数据列名是字符串)\n",
        "    data_columns_str = [str(col) for col in data_df.columns]\n",
        "    valid_cols = [col for col in columns if col in data_columns_str]\n",
        "\n",
        "    if valid_cols:\n",
        "        valid_factor_groups[group_name] = valid_cols\n",
        "        # 避免重复添加列\n",
        "        for col in valid_cols:\n",
        "            if col not in all_low_level_columns:\n",
        "                all_low_level_columns.append(col)\n",
        "        print(f\"因子组 '{group_name}': 找到 {len(valid_cols)} 个有效列 (例如: {valid_cols[:3]}...)\")\n",
        "    else:\n",
        "        print(f\"警告: 因子组 '{group_name}' 没有在数据 ('{data_file}') 中找到任何对应的列。\")\n",
        "        print(f\"  尝试匹配的列名示例: {columns[:5]}\")\n",
        "        print(f\"  '{data_file}' 中的列名示例: {data_columns_str[:5]}\")\n",
        "\n",
        "\n",
        "# 确保至少找到了一些低层因子列\n",
        "if not all_low_level_columns:\n",
        "     print(f\"\\n错误: 未能在数据文件 ('{data_file}') 中根据因子说明找到任何有效的低层因子列。\")\n",
        "     print(\"请检查 '因子说明.csv' 中的 '变量名' 列是否与 '{data_file}' 中的列名完全匹配。\")\n",
        "     exit()\n",
        "else:\n",
        "    print(f\"\\n共找到 {len(all_low_level_columns)} 个有效的低层因子列用于模型输入。\")\n",
        "\n",
        "\n",
        "target_column = '满意度'\n",
        "# 检查目标列是否存在 (也转为字符串比较以防万一)\n",
        "if str(target_column) not in [str(col) for col in data_df.columns]:\n",
        "    print(f\"错误: 目标列 '{target_column}' 不在数据文件 ('{data_file}') 中!\")\n",
        "    exit()\n",
        "\n",
        "# 提取特征和目标\n",
        "X_low_level = data_df[all_low_level_columns].copy()\n",
        "y = data_df[target_column].copy()\n",
        "\n",
        "# --- 2. 数据预处理 ---\n",
        "\n",
        "# a. 处理缺失值 (使用中位数填充)\n",
        "if X_low_level.empty:\n",
        "    print(\"错误：特征数据框为空，无法进行缺失值填充。\")\n",
        "    exit()\n",
        "\n",
        "# 确保所有特征列都是数值类型，尝试转换，如果失败则报错\n",
        "try:\n",
        "    X_low_level = X_low_level.astype(float)\n",
        "except ValueError as e:\n",
        "    print(f\"错误: 特征列包含无法转换为数值的数据: {e}\")\n",
        "    # 找出哪些列有问题\n",
        "    non_numeric_cols = []\n",
        "    for col in X_low_level.columns:\n",
        "        try:\n",
        "            pd.to_numeric(X_low_level[col])\n",
        "        except ValueError:\n",
        "            non_numeric_cols.append(col)\n",
        "    print(f\"无法转换为数值的列: {non_numeric_cols}\")\n",
        "    print(\"请检查这些列在 data_df 中的原始数据类型和内容。\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "try:\n",
        "    X_low_level = pd.DataFrame(imputer.fit_transform(X_low_level), columns=all_low_level_columns)\n",
        "    print(\"\\n缺失值已用中位数填充。\")\n",
        "except ValueError as e:\n",
        "    print(f\"错误：在填充缺失值时发生错误: {e}\")\n",
        "    print(\"请再次检查特征列是否只包含数值数据。\")\n",
        "    exit()\n",
        "\n",
        "if y.isnull().any():\n",
        "    print(f\"目标变量 '{target_column}' 存在缺失值，用中位数填充。\")\n",
        "    y.fillna(y.median(), inplace=True)\n",
        "\n",
        "# b. 标准化数值特征\n",
        "if X_low_level.empty:\n",
        "    print(\"错误：特征数据框为空，无法进行标准化。\")\n",
        "    exit()\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_low_level)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=all_low_level_columns) # 转回DataFrame方便按组取列\n",
        "print(\"特征数据已标准化。\")\n",
        "\n",
        "# --- 3. 准备PyTorch数据 ---\n",
        "X_tensor = torch.tensor(X_scaled_df.values.astype(np.float32))\n",
        "y_tensor = torch.tensor(y.values.astype(np.float32)).unsqueeze(1)\n",
        "\n",
        "# 划分训练集和测试集\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"\\n训练集大小: {X_train.shape[0]}, 测试集大小: {X_test.shape[0]}\")\n",
        "print(f\"特征维度: {X_train.shape[1]}\")\n",
        "\n",
        "# 创建DataLoader\n",
        "batch_size = 64\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# --- 4. 构建HAN模型 ---\n",
        "\n",
        "# 定义简单的注意力层 (与之前版本相同)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attention_fc = nn.Linear(feature_dim, attention_dim)\n",
        "        self.context_vector = nn.Parameter(torch.Tensor(attention_dim, 1))\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self._reset_parameters()\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.attention_fc.weight)\n",
        "        nn.init.zeros_(self.attention_fc.bias)\n",
        "        nn.init.xavier_uniform_(self.context_vector)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, feature_dim)\n",
        "        attn_hidden = torch.tanh(self.attention_fc(x))\n",
        "        # scores shape: (batch_size, seq_len, 1)\n",
        "        scores = attn_hidden @ self.context_vector\n",
        "        # attention_weights shape: (batch_size, seq_len, 1)\n",
        "        attention_weights = self.softmax(scores)\n",
        "        # context shape: (batch_size, feature_dim)\n",
        "        context = torch.sum(attention_weights * x, dim=1)\n",
        "        return context, attention_weights.squeeze(-1)\n",
        "\n",
        "# 定义层次化注意力网络 (HAN) (与之前版本相同，但依赖于正确的 factor_groups_indices)\n",
        "class HANRegression(nn.Module):\n",
        "    def __init__(self, factor_groups_indices, low_level_feature_dim,\n",
        "                 factor_attention_dim=64, factor_output_dim=64,\n",
        "                 overall_attention_dim=64, dropout_rate=0.3):\n",
        "        super(HANRegression, self).__init__()\n",
        "        self.factor_groups_indices = factor_groups_indices\n",
        "        self.num_high_level_factors = len(factor_groups_indices)\n",
        "        self.low_level_feature_dim = low_level_feature_dim # This is the EMBEDDING dim for level 1\n",
        "\n",
        "        # Layer 1: Factor-level Attention\n",
        "        # We need an embedding layer if input feature dim is 1\n",
        "        self.embedding_dim = 16 # Define the embedding dimension for input features (if they are 1D)\n",
        "        self.embedding_layer = nn.Linear(1, self.embedding_dim) # Simple linear layer acts as embedding\n",
        "\n",
        "        self.factor_attentions = nn.ModuleDict({\n",
        "            # Attention layer now operates on the embedding dimension\n",
        "            group_name: Attention(self.embedding_dim, factor_attention_dim)\n",
        "            for group_name in factor_groups_indices.keys()\n",
        "        })\n",
        "        self.factor_linears = nn.ModuleDict({\n",
        "             # Input dim is embedding_dim (output of Attention)\n",
        "             group_name: nn.Linear(self.embedding_dim, factor_output_dim)\n",
        "             for group_name in factor_groups_indices.keys()\n",
        "        })\n",
        "        self.factor_relu = nn.ReLU()\n",
        "        self.factor_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Layer 2: Overall Attention\n",
        "        self.overall_attention = Attention(factor_output_dim, overall_attention_dim)\n",
        "\n",
        "        # Output Layer\n",
        "        self.output_layer = nn.Linear(factor_output_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, total_low_level_features)\n",
        "        factor_contexts = []\n",
        "        factor_group_attentions = {}\n",
        "\n",
        "        # Layer 1 Attention: Process each factor group\n",
        "        for group_name, indices in self.factor_groups_indices.items():\n",
        "            x_group = x[:, indices] # (batch_size, num_features_in_group)\n",
        "            if x_group.shape[1] == 0: continue\n",
        "\n",
        "            # Reshape and Embed: Treat each feature score as a sequence element with feature_dim=1\n",
        "            x_group_seq = x_group.unsqueeze(-1) # (batch_size, num_features_in_group, 1)\n",
        "            x_group_embedded = self.embedding_layer(x_group_seq) # (batch_size, num_features_in_group, embedding_dim)\n",
        "\n",
        "            # Apply factor-level attention on embedded features\n",
        "            context, attn1 = self.factor_attentions[group_name](x_group_embedded) # context shape: (batch_size, embedding_dim)\n",
        "\n",
        "            # Apply linear layer after attention\n",
        "            processed_context = self.factor_dropout(self.factor_relu(self.factor_linears[group_name](context))) # (batch_size, factor_output_dim)\n",
        "            factor_contexts.append(processed_context)\n",
        "            factor_group_attentions[group_name] = attn1\n",
        "\n",
        "        if not factor_contexts:\n",
        "             # This should not happen if all_low_level_columns is not empty, but check anyway\n",
        "             print(\"警告: 在前向传播中没有有效的因子上下文生成。\")\n",
        "             # Return zeros or handle appropriately\n",
        "             return torch.zeros(x.shape[0], 1).to(x.device), None # Return dummy output and None weights\n",
        "\n",
        "        # Stack context vectors\n",
        "        factor_contexts_tensor = torch.stack(factor_contexts, dim=1) # (batch_size, num_high_level_factors, factor_output_dim)\n",
        "\n",
        "        # Layer 2 Attention\n",
        "        overall_context, overall_attention_weights = self.overall_attention(factor_contexts_tensor) # (batch_size, factor_output_dim)\n",
        "\n",
        "        # Output Layer\n",
        "        output = self.output_layer(overall_context) # (batch_size, 1)\n",
        "\n",
        "        return output, overall_attention_weights\n",
        "\n",
        "\n",
        "# 获取每个因子组对应的列索引 (基于 all_low_level_columns 的顺序)\n",
        "factor_groups_indices = {}\n",
        "current_columns_in_tensor = all_low_level_columns # 特征张量中的列顺序与此列表一致\n",
        "for group_name, group_cols in valid_factor_groups.items():\n",
        "    # 找到 group_cols 在 current_columns_in_tensor 中的索引\n",
        "    indices = [current_columns_in_tensor.index(col) for col in group_cols if col in current_columns_in_tensor]\n",
        "    if indices:\n",
        "        factor_groups_indices[group_name] = indices\n",
        "    else:\n",
        "         print(f\"警告: 因子组 '{group_name}' 在最终特征列表中的索引为空，可能在过滤时出现问题。\")\n",
        "\n",
        "\n",
        "# 检查是否有有效的因子组索引\n",
        "if not factor_groups_indices:\n",
        "    print(\"\\n错误: 未能为任何因子组生成有效的列索引。无法实例化模型。\")\n",
        "    exit()\n",
        "\n",
        "# 实例化模型\n",
        "# low_level_feature_dim is the dimension *after* embedding\n",
        "model = HANRegression(factor_groups_indices, low_level_feature_dim=16, # Corresponds to self.embedding_dim\n",
        "                      factor_attention_dim=32, factor_output_dim=64,\n",
        "                      overall_attention_dim=32)\n",
        "\n",
        "print(\"\\nHAN 模型结构 (概要):\")\n",
        "print(f\"Number of high-level factors: {model.num_high_level_factors}\")\n",
        "print(f\"Embedding dim for low-level features: {model.embedding_dim}\")\n",
        "print(f\"Factor attention dim: 32, Factor output dim: 64\")\n",
        "print(f\"Overall attention dim: 32\")\n",
        "\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"模型将在 {device} 上运行。\")\n",
        "\n",
        "\n",
        "# --- 5. 定义损失函数和优化器 ---\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# --- 6. 训练模型 ---\n",
        "num_epochs = 30\n",
        "print(f\"\\n开始训练模型，共 {num_epochs} 个周期...\")\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "best_test_loss = float('inf')\n",
        "model_save_path = 'best_han_model.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs, _ = model(inputs)\n",
        "        # Check for NaN in outputs or labels\n",
        "        if torch.isnan(outputs).any() or torch.isnan(labels).any():\n",
        "            print(f\"警告: 周期 {epoch+1} 检测到 NaN 值。跳过此批次。\")\n",
        "            continue\n",
        "        loss = criterion(outputs, labels)\n",
        "        if torch.isnan(loss):\n",
        "             print(f\"警告: 周期 {epoch+1} 损失为 NaN。跳过梯度更新。\")\n",
        "             continue\n",
        "        loss.backward()\n",
        "        # Optional: Gradient clipping to prevent exploding gradients\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset) if len(train_loader.dataset) > 0 else 0\n",
        "    train_losses.append(epoch_train_loss)\n",
        "\n",
        "    # Evaluate on the test set\n",
        "    model.eval()\n",
        "    running_test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, _ = model(inputs)\n",
        "            if torch.isnan(outputs).any() or torch.isnan(labels).any():\n",
        "                 print(f\"警告: 测试时检测到 NaN 值。\")\n",
        "                 continue # Skip this batch if NaN occurs\n",
        "            loss = criterion(outputs, labels)\n",
        "            if torch.isnan(loss):\n",
        "                 print(f\"警告: 测试损失为 NaN。\")\n",
        "                 continue\n",
        "            running_test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    epoch_test_loss = running_test_loss / len(test_loader.dataset) if len(test_loader.dataset) > 0 else float('inf')\n",
        "    test_losses.append(epoch_test_loss)\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        print(f\"周期 [{epoch+1}/{num_epochs}], 训练损失: {epoch_train_loss:.4f}, 测试损失: {epoch_test_loss:.4f}\")\n",
        "\n",
        "    # Save the best model based on test loss (and ensure loss is not NaN)\n",
        "    if not np.isnan(epoch_test_loss) and epoch_test_loss < best_test_loss:\n",
        "        best_test_loss = epoch_test_loss\n",
        "        try:\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "        except Exception as e:\n",
        "            print(f\"保存模型时出错: {e}\")\n",
        "\n",
        "print(\"模型训练完成。\")\n",
        "\n",
        "# --- 7. 评估模型 (使用最佳模型) ---\n",
        "if best_test_loss == float('inf'):\n",
        "    print(\"\\n警告: 未能成功训练模型或保存最佳模型（测试损失始终为无穷大或NaN）。无法进行评估。\")\n",
        "else:\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(model_save_path))\n",
        "        print(f\"\\n已加载最佳模型 (测试损失: {best_test_loss:.4f}) 进行最终评估。\")\n",
        "    except Exception as e:\n",
        "        print(f\"加载最佳模型失败: {e}. 使用最终训练的模型进行评估。\")\n",
        "\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_attn_weights = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs, attn_weights = model(inputs)\n",
        "            # Check for NaN before extending lists\n",
        "            if not torch.isnan(outputs).any() and attn_weights is not None and not torch.isnan(attn_weights).any():\n",
        "                all_preds.extend(outputs.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_attn_weights.extend(attn_weights.cpu().numpy())\n",
        "            else:\n",
        "                print(\"警告: 评估时跳过包含 NaN 的批次输出或注意力权重。\")\n",
        "\n",
        "\n",
        "    if not all_preds:\n",
        "        print(\"\\n错误: 未能生成任何有效的预测结果，无法进行评估。\")\n",
        "    else:\n",
        "        all_preds = np.array(all_preds).flatten()\n",
        "        all_labels = np.array(all_labels).flatten()\n",
        "        if all_attn_weights:\n",
        "            all_attn_weights = np.array(all_attn_weights)\n",
        "        else:\n",
        "             all_attn_weights = np.empty((0, model.num_high_level_factors))\n",
        "\n",
        "\n",
        "        mse = mean_squared_error(all_labels, all_preds)\n",
        "        rmse = np.sqrt(mse)\n",
        "        r2 = r2_score(all_labels, all_preds)\n",
        "\n",
        "        print(\"\\n模型在测试集上的最终评估结果:\")\n",
        "        print(f\"均方误差 (MSE): {mse:.4f}\")\n",
        "        print(f\"均方根误差 (RMSE): {rmse:.4f}\")\n",
        "        print(f\"R² 分数: {r2:.4f}\")\n",
        "\n",
        "        # --- 8. 可解释性分析 (第二层注意力) ---\n",
        "        if all_attn_weights.shape[0] > 0:\n",
        "            avg_attn_weights = np.mean(all_attn_weights, axis=0)\n",
        "            # Get factor names in the order they appear in the model's final processing\n",
        "            # This order corresponds to the order in factor_groups_indices keys\n",
        "            high_level_factor_names = list(factor_groups_indices.keys())\n",
        "\n",
        "            print(\"\\n高层因子（第二层）平均注意力权重:\")\n",
        "            sorted_indices = np.argsort(avg_attn_weights)[::-1]\n",
        "            for i in sorted_indices:\n",
        "                # Ensure index is within bounds\n",
        "                if i < len(high_level_factor_names):\n",
        "                     print(f\"- {high_level_factor_names[i]}: {avg_attn_weights[i]:.4f}\")\n",
        "                else:\n",
        "                     print(f\"警告: 排序索引 {i} 超出因子名称列表范围。\")\n",
        "\n",
        "\n",
        "            # Visualize attention weights\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            # Ensure sorted_names matches sorted_weights length\n",
        "            valid_sorted_indices = [i for i in sorted_indices if i < len(high_level_factor_names)]\n",
        "            sorted_names = [high_level_factor_names[i] for i in valid_sorted_indices]\n",
        "            sorted_weights = avg_attn_weights[valid_sorted_indices]\n",
        "\n",
        "            if len(sorted_names) > 0: # Check if there are names to plot\n",
        "                plt.bar(sorted_names, sorted_weights, color='skyblue')\n",
        "                plt.xlabel(\"高层因子 (按重要性排序)\")\n",
        "                plt.ylabel(\"平均注意力权重\")\n",
        "                plt.title(\"高层因子对满意度预测的平均重要性 (第二层注意力)\")\n",
        "                plt.xticks(rotation=45, ha='right')\n",
        "                plt.tight_layout()\n",
        "                # plt.show()\n",
        "                try:\n",
        "                    plt.savefig(\"han_factor_attention_weights.png\")\n",
        "                    print(\"注意力权重图已保存为 han_factor_attention_weights.png\")\n",
        "                except Exception as e:\n",
        "                    print(f\"保存注意力权重图时出错: {e}\")\n",
        "            else:\n",
        "                print(\"警告: 没有有效的排序因子名称用于绘图。\")\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"\\n未能收集到注意力权重进行分析。\")\n",
        "\n",
        "\n",
        "# --- 9. 结论与后续 ---\n",
        "print(\"\\n--- 结论与后续步骤 ---\")\n",
        "print(\"1. 更新了代码以尝试从 '因子说明.csv' 动态构建因子分组。\")\n",
        "print(\"2. **重要**: 请确认 '因子说明.csv' 文件中包含名为 '一级指标' 和 '变量名' 的列，并且 '变量名' 列的内容与 'case1_data.csv' 中的实际列名匹配。如果列名不同，请修改代码中读取这些列的部分。\")\n",
        "print(\"3. 增加了对特征列数据类型的检查和转换，以确保它们是数值类型。\")\n",
        "print(\"4. 调整了HAN模型结构，明确加入了Embedding层处理输入的1维特征。\")\n",
        "print(\"5. 增加了训练过程中的NaN值检查，以提高稳定性。\")\n",
        "print(\"6. 如果动态分组仍然失败或不准确，您可能需要手动检查 `case1_data.csv` 的列名，并在代码中直接定义 `high_level_factors_map`。\")\n",
        "print(\"7. 模型性能和解释性分析依赖于正确的分组和训练过程的稳定。\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "vvoB0vrBofFV"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}